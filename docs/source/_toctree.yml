

- sections: 
  - local: index
    title: Inference Endpoints
  - local: quick_start
    title: Quick Start 
  - local: pricing
    title: Pricing
  - local: support
    title: Help & Support
  - local: faq
    title: FAQ
  - local: api_reference
    title: API Reference
  title: Overview
- sections:
  - local: guides/foundations
    title: Foundations
  - local: guides/configuration
    title: Configuration
  - local: guides/autoscaling
    title: Auto Scaling
  - local: guides/logs
    title: Logs
  - local: guides/metrics
    title: Metrics
  - local: guides/openmetrics
    title: Export Inference Endpoints Metrics to your internal tool 
  - local: guides/security
    title: Security & Compliance
  - local: guides/private_link
    title: Private Endpoints with AWS PrivateLink
  - local: guides/custom_container
    title: Use a custom Container Image
  - local: guides/programatically
    title: Manage Inference Endpoints programatically
  title: Guides
- sections:
  - local: engines/tgi
    title: Text Generation Inference (TGI)
  - local: engines/vllm
    title: vLLM 
  - local: engines/tei
    title: Text Embedding Inference (TEI)
  - local: engines/llama_cpp
    title: llama.cpp 
  - local: engines/default
    title: Default
  title: Inference Engines
- sections: 
  - local: tutorials/chat_bot
    title: Deploy your own chat application
  - local: tutorials/embedding
    title: Build an embedding pipeline
  - local: tutorials/transcription
    title: Create your own transcription app 
  title: Tutorials
