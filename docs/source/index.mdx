# Inference Endpoints

Inference Endpoints offers a secure production solution to easily deploy any Transformers, Sentence-Transformers and Diffusers models from the Hub on dedicated and autoscaling infrastructure managed by Hugging Face.

A Hugging Face Endpoint is built from a [Hugging Face Model Repository](https://huggingface.co/models). When an Endpoint is created, the service creates image artifacts that are either built from the model you select or a custom-provided container image. The image artifacts are completely decoupled from the Hugging Face Hub source repositories to ensure the highest security and reliability levels.

Inference Endpoints support all of the [Transformers, Sentence-Transformers and Diffusers tasks](/docs/inference-endpoints/supported_tasks) as well as [custom tasks](/docs/inference-endpoints/guides/custom_handler) not supported by Transformers yet like speaker diarization and diffusion.

In addition, Inference Endpoints gives you the option to use a custom container image managed on an external service, for instance, [Docker Hub](https://hub.docker.com/), [AWS ECR](https://aws.amazon.com/ecr/?nc1=h_ls), [Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/), or [Google GCR](https://cloud.google.com/container-registry?hl=de). 

![creation-flow](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/creation_flow.png)

## Documentation and Examples

* [Security & Compliance](/docs/inference-endpoints/security)
* [Supported Transformers Task](/docs/inference-endpoints/supported_tasks)
* [API Reference](/docs/inference-endpoints/api_reference)
* [Autoscaling](/docs/inference-endpoints/autoscaling)
* [FAQ](/docs/inference-endpoints/faq)
* [Help & Support](/docs/inference-endpoints/support)

### Guides

* [Access the solution (UI)](/docs/inference-endpoints/guides/access)
* [Create your first Endpoint](/docs/inference-endpoints/guides/create_endpoint)
* [Send Requests to Endpoints](/docs/inference-endpoints/guides/test_endpoint)
* [Update your Endpoint](/docs/inference-endpoints/guides/update_endpoint)
* [Advanced Setup (Instance Types, Auto Scaling, Versioning)](/docs/inference-endpoints/guides/advanced)
* [Create a Private Endpoint with AWS PrivateLink](/docs/inference-endpoints/guides/private_link)
* [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies)
* [Create custom Inference Handler](/docs/inference-endpoints/guides/custom_handler)
* [Use a custom Container Image](/docs/inference-endpoints/guides/custom_container)
* [Access and read Logs](/docs/inference-endpoints/guides/logs)
* [Access and view Metrics](/docs/inference-endpoints/guides/metrics)
* [Change Organization or Account](/docs/inference-endpoints/guides/change_organization)

### Others

* [Inference Endpoints Versions](/docs/inference-endpoints/others/runtime)
* [Serialization & Deserialization for Requests](/docs/inference-endpoints/others/serialization)
