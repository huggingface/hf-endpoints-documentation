# Quick Start

In this guide you'll deploy a production ready AI model using Inference Endpoints in only a few minutes.
Make sure you've been able to log into the [Inference Endpoints UI](https://endpoints.huggingface.co) with your Hugging Face account, and that you have a payment
method setup. If not you can add a payment method [from this link](https://huggingface.co/settings/billing).

## Create your endpoint

Start by navigating to the Inference Endpoints UI, and once you're logged in, you should see a button for creating a new Inference
Endpoint, and a small greeting prompting you to create your first endpoint. Click the "New" button.

![new-button](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/update/large-rewrite/assets/quick_start/1-new-button.png)

From there you'll be directed to the catalog. The Model Catalog consists of popular models which have tuned configurations to work just as one-click
deploys. You can filter by name, task, hardware price, and much more.

![catalog](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/update/large-rewrite/assets/quick_start/2-catalog.png)

In this example let's deploy the [meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) model. You can find
it by searching for `llama-3.2-3b` in the search field and deploy it by clicking the card.

![llama](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/update/large-rewrite/assets/quick_start/3-llama.png)

Next we'll choose which hardware and deployment settings we'll go for. Since this is a catalog model, all of the pre-selected options are very good
defaults. So in this case we don't need to change anything. In case you want a deeper dive on what the different settings mean you can check out
the [configuration guide](./guides/configuration).

For this model the Nvidia L4 is the recommended choice. It will be perfect for our testing. Performant but still reasonably priced. Also note that by
default the endpoint will scale down to zero, meaning it will become idle after 1h of inactivity.

Now all you need to do is click click "Create Endpoint" ðŸš€

![config](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/update/large-rewrite/assets/quick_start/4-config.png)

Now our Inference Endpoint is initializing, which usually takes about 3-5 minutes. If you want to can allow browser notifications which will give you a
ping once the endpoint reaches a running state.

![init](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/update/large-rewrite/assets/quick_start/5-init.png)

## Test your endpoint

And then once everything is up and running you'll be able to see the:
- **Endpoint URL**: this is what you use to call your endpoint and send requests to it
- **Playground**: a small visual way of quickly testing that the model works

![done](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/update/large-rewrite/assets/quick_start/6-done.png)

From the side of the playground you can also copy + paste a code snippet for calling the model. By clicking "App Tokens" you'll be directed to Hugging Face
to configure an access token to be able to call the model. By default, all Inference Endpoints are created as private once which require authentication and
all data is encryped in transit using TLS/SSL.

Congratulrations, you just deployed a production ready AI model ðŸ”¥

Once you're happy with the testing you can pause the model, delete it. Or if you let it be, it will become idle after 1 hour.

