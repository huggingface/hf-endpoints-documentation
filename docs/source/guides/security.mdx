# Security & Compliance

Inference Endpoints are built with security and secure inference at their core. Below you can find an overview of the security measures
we have in place.

## Data Security & Privacy

Hugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint.
We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.

We also recommend using AWS Private Link for organizations. This allows you to access your Inference Endpoint through a
private connection, without exposing it to the internet.

Hugging Face also offers GDPR data processing agreement through an Enterprise Hub subscription. For more information or to
subscribe to Enterprise Hub, please visit https://huggingface.co/enterprise.

## Model Security & Privacy

You can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or
data you upload to the Hugging Face Hub. Hugging Face also provides malware and pickle scans over the contents of the model
repository as with all items in the Hub.

## Inference Endpoints and Hub Security

The Hugging Face Hub and Inference Endpoints are SOC2 Type 2 certified. The Hugging Face Hub also offers Role Based Access Control. 

You can read more about security at Hugging Face in general in the following links:
- information on Hugging Face Hub security: https://huggingface.co/docs/hub/security. 
- information on the Enterprise Hub subscription and its premium security features: https://huggingface.co/docs/hub/enterprise-hub

<div class="flex justify-center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg" alt="soc-1" class="max-w-[300px] w-full" />
</div>

## Inference Endpoint Security level

We currently offer four ways of securing your Inference Endpoints:

- **Public**: A Public Endpoint is available from the internet, secured with TLS/SSL, and requires no authentication.
- **HF Restricted**: The Inference Endpoint is available from the Internet, and secured with TLS/SSL. Anyone with a Hugging Face account can access it, using a personal Hugging Face Token generated from their account.
- **Protected**: A Protected Endpoint is available from the internet, secured with TLS/SSL, and requires a valid Hugging Face token for authentication.
- **Private**: A Private Endpoint is only available through an intra-region secured AWS or Azure PrivateLink connection. Private Endpoints are not accessible from the internet.

Public, Protected, and HF Restricted Endpoints do not require any additional configuration. For Private Endpoints, you need to provide
the AWS account ID of the account that should also have access to Inference Endpoints.

![endpoint-types](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/endpoint_types.png)

## Further information

You can read the Hugging Face Privacy Policy at: https://huggingface.co/privacy
